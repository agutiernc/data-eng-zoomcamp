{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78fff57b-44e0-435b-972c-4e1d8b42d4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time \n",
    "\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "def json_serializer(data):\n",
    "    return json.dumps(data).encode('utf-8')\n",
    "\n",
    "server = 'localhost:9092'\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=[server],\n",
    "    value_serializer=json_serializer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b1a069e-245e-4fb5-a7cd-7e6691c99696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "producer.bootstrap_connected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "041233f8-ce48-4109-8d0e-7ce950e0aec0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent: {'number': 0}\n",
      "Sent: {'number': 1}\n",
      "Sent: {'number': 2}\n",
      "Sent: {'number': 3}\n",
      "Sent: {'number': 4}\n",
      "Sent: {'number': 5}\n",
      "Sent: {'number': 6}\n",
      "Sent: {'number': 7}\n",
      "Sent: {'number': 8}\n",
      "Sent: {'number': 9}\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "topic_name = 'test-topic'\n",
    "\n",
    "for i in range(10):\n",
    "    message = {'number': i}\n",
    "    producer.send(topic_name, value=message)\n",
    "    print(f\"Sent: {message}\")\n",
    "    time.sleep(0.05)\n",
    "\n",
    "producer.flush()\n",
    "\n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06357325-0ece-42bc-9878-1764ff2f7b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 0.51 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f'took {(t1 - t0):.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d230750a-d77d-40dd-9edd-9cd869c9c8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cbfbdff-a4f0-4a8b-a420-28d205ff3624",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/macmint/anaconda3/envs/venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3553: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"green_tripdata_2019-10.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4467fb38-c8ef-4539-a1f3-8769d80d1853",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green = df[['lpep_pickup_datetime', 'lpep_dropoff_datetime', 'PULocationID', 'DOLocationID', 'passenger_count', 'trip_distance', 'tip_amount']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed41ef2c-76a9-46bc-9acc-7e2a1a90ac99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data sent in 60.09 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "for row in df_green.itertuples(index=False):\n",
    "    row_dict = {col: getattr(row, col) for col in row._fields}\n",
    "    producer.send(\"green-trips\", value=row_dict)\n",
    "\n",
    "producer.flush()\n",
    "t1 = time.time()\n",
    "print(f'data sent in {(t1 - t0):.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2967a022-3740-42c2-9fc1-74843f01089d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e00dd147-6e5f-49ab-ad0e-3f0d51544488",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark_version = pyspark.__version__\n",
    "kafka_jar_package = f\"org.apache.spark:spark-sql-kafka-0-10_2.12:{pyspark_version}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92410530-36a6-4ce0-b461-f88a3dfacf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"GreenTripsConsumer\") \\\n",
    "    .config(\"spark.jars.packages\", kafka_jar_package) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f1821f9-5372-4cd3-8f30-7e5175153689",
   "metadata": {},
   "outputs": [],
   "source": [
    "green_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"green-trips\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5fd6d5e4-39f2-46f7-9ddc-ae6d6d6e8c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def peek(mini_batch, batch_id):\n",
    "    first_row = mini_batch.take(1)\n",
    "\n",
    "    if first_row:\n",
    "        print(first_row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca4bcec9-b425-495c-97f3-c1b38ae6e697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/07 18:44:26 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-3fa513cf-7380-45a5-87fc-153727a4ff2a. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/07 18:44:27 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(key=None, value=bytearray(b'{\"lpep_pickup_datetime\": \"2019-10-01 00:26:02\", \"lpep_dropoff_datetime\": \"2019-10-01 00:39:58\", \"PULocationID\": 112, \"DOLocationID\": 196, \"passenger_count\": 1.0, \"trip_distance\": 5.88, \"tip_amount\": 0.0}'), topic='green-trips', partition=0, offset=0, timestamp=datetime.datetime(2024, 4, 7, 15, 47, 38, 655000), timestampType=0)\n"
     ]
    }
   ],
   "source": [
    "query = green_stream.writeStream.foreachBatch(peek).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "880b69b7-814e-47a0-bc4b-72103fa0124a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f00825c-b16b-46a9-9403-6014fec65e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f7ee6c1-ebf1-4aa4-9828-f0386cc7656e",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType() \\\n",
    "    .add(\"lpep_pickup_datetime\", types.StringType()) \\\n",
    "    .add(\"lpep_dropoff_datetime\", types.StringType()) \\\n",
    "    .add(\"PULocationID\", types.IntegerType()) \\\n",
    "    .add(\"DOLocationID\", types.IntegerType()) \\\n",
    "    .add(\"passenger_count\", types.DoubleType()) \\\n",
    "    .add(\"trip_distance\", types.DoubleType()) \\\n",
    "    .add(\"tip_amount\", types.DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dee5b96a-3818-4870-8f6e-be4af8bd304a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce50c772-cf09-4213-9846-bc75aed7a8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "green_stream = green_stream \\\n",
    "  .select(F.from_json(F.col(\"value\").cast('STRING'), schema).alias(\"data\")) \\\n",
    "  .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3848b9b6-b7cb-4a24-9ce9-0490930bdd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/07 18:49:06 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-9f681eba-b15c-4e02-bdac-90ffaedaf913. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/07 18:49:06 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "Row(lpep_pickup_datetime='2019-10-01 00:26:02', lpep_dropoff_datetime='2019-10-01 00:39:58', PULocationID=112, DOLocationID=196, passenger_count=1.0, trip_distance=5.88, tip_amount=0.0)\n"
     ]
    }
   ],
   "source": [
    "# to see what the record looks like after parsing\n",
    "query = green_stream.writeStream.foreachBatch(peek).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2250345b-92f0-46fe-9550-05a1c3909b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35725274-88df-4ea2-a9d6-a6ea78b0540c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding timestamp column\n",
    "green_stream = green_stream.withColumn(\"timestamp\", F.current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8beb8a78-0e3f-4f1f-bd94-790494ff0e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_green_stream = green_stream \\\n",
    "    .groupBy(F.window(F.col(\"timestamp\"), \"5 minutes\"), F.col(\"DOLocationID\")) \\\n",
    "    .count() \\\n",
    "    .orderBy(F.col(\"count\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0fede6-3fbc-4f57-929f-36f043dd776a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/07 19:00:17 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-70e5643c-33ef-49e9-9131-e4929dd7b913. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/07 19:00:17 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------------------------------------------+------------+-----+\n",
      "|window                                    |DOLocationID|count|\n",
      "+------------------------------------------+------------+-----+\n",
      "|{2024-04-07 19:00:00, 2024-04-07 19:05:00}|74          |35482|\n",
      "|{2024-04-07 19:00:00, 2024-04-07 19:05:00}|42          |31884|\n",
      "|{2024-04-07 19:00:00, 2024-04-07 19:05:00}|41          |28122|\n",
      "|{2024-04-07 19:00:00, 2024-04-07 19:05:00}|75          |25680|\n",
      "|{2024-04-07 19:00:00, 2024-04-07 19:05:00}|129         |23860|\n",
      "|{2024-04-07 19:00:00, 2024-04-07 19:05:00}|7           |23066|\n",
      "|{2024-04-07 19:00:00, 2024-04-07 19:05:00}|166         |21690|\n",
      "|{2024-04-07 19:00:00, 2024-04-07 19:05:00}|236         |15826|\n",
      "|{2024-04-07 19:00:00, 2024-04-07 19:05:00}|223         |15084|\n",
      "|{2024-04-07 19:00:00, 2024-04-07 19:05:00}|238         |14636|\n",
      "|{2024-04-07 19:00:00, 2024-04-07 19:05:00}|82          |14584|\n",
      "|{2024-04-07 19:00:00, 2024-04-07 19:05:00}|181         |14564|\n",
      "|{2024-04-07 19:00:00, 2024-04-07 19:05:00}|95          |14488|\n",
      "|{2024-04-07 19:00:00, 2024-04-07 19:05:00}|244         |13466|\n",
      "|{2024-04-07 19:00:00, 2024-04-07 19:05:00}|61          |13212|\n",
      "|{2024-04-07 19:00:00, 2024-04-07 19:05:00}|116         |12678|\n",
      "|{2024-04-07 19:00:00, 2024-04-07 19:05:00}|138         |12288|\n",
      "|{2024-04-07 19:00:00, 2024-04-07 19:05:00}|97          |12100|\n",
      "|{2024-04-07 19:00:00, 2024-04-07 19:05:00}|49          |10442|\n",
      "|{2024-04-07 19:00:00, 2024-04-07 19:05:00}|151         |10306|\n",
      "+------------------------------------------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = popular_destinations \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae5b7ae-f366-4b71-838d-ae416404fcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
